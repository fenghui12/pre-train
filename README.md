# LLM 微调与管理助手

本项目是一个基于 Tkinter 的图形化桌面应用，旨在为用户提供一个完整、易于操作的本地大型语言模型（LLM）微调与管理工作流。它涵盖了从使用自定义数据进行 LoRA 微调、模型推理测试，到最终将模型合并、转换为 GGUF 格式并导入 Ollama 的全过程，极大地简化了本地化模型的训练与部署流程。

## ✨ 核心功能

- **图形化界面**: 提供直观的图形用户界面，用户无需编写代码即可完成所有操作。
- **LoRA 微调**:
    - 支持从零开始训练新的 LoRA 适配器。
    - 支持在现有 LoRA 适配器的基础上进行增量训练。
    - 训练过程采用 4-bit 量化，有效降低显存消耗。
- **模型推理与对话**:
    - 内置聊天界面，可加载基座模型或已合并的 LoRA 模型进行对话测试。
    - 支持上下文对话模式。
- **Gradio 一键分享**:
    - 加载模型后，可一键启动 Gradio 服务，生成公网链接。
    - 方便地将本地模型分享给他人进行远程访问和测试。
- **模型合并与转换**:
    - 可将训练好的 LoRA 适配器与基座模型进行合并。
    - 支持将 Hugging Face 格式的基座模型或合并后的模型，转换为 Ollama 所需的 GGUF 格式。
- **Ollama 集成**:
    - 自动化创建 Modelfile 并调用 Ollama 命令，将转换后的 GGUF 模型一键导入到本地 Ollama 服务中。

## ⚙️ 环境依赖与准备

### 1. Python 库

项目依赖以下 Python 库。您可以通过 `requirements.txt` 文件一键安装：

```bash
pip install -r requirements.txt
```

主要库包括:
- `gradio`
- `torch`
- `transformers`
- `accelerate`
- `peft`
- `datasets`

### 2. 外部依赖

为了使用完整的模型转换和导入功能，您需要预先在您的系统上安装和配置好以下软件：

- **Ollama**: 用于运行和管理本地大模型。请从 [Ollama 官网](https://ollama.com/) 下载并安装。
- **Git**: 用于克隆 `llama.cpp` 仓库。
- **llama.cpp**: 用于将 Hugging Face 模型转换为 GGUF 格式。
    - 请使用 Git 克隆该仓库到您的本地任意位置：
      ```bash
      git clone https://github.com/ggerganov/llama.cpp.git
      ```
    - **重要**: 克隆后，请在本应用的 **“设置”** 选项卡中，配置 `llama.cpp` 仓库的本地路径。

## 🚀 如何使用

1.  **启动应用**:
    ```bash
    python main_app.py
    ```

2.  **训练 (Train)**:
    - 在“训练”选项卡中，选择“新 LoRA 训练”或“继续训练”。
    - 选择一个本地缓存的基座模型或一个已有的 LoRA 模型目录。
    - 选择您的 `.jsonl` 格式训练数据集。
    - 指定一个输出目录，点击“开始训练”。
    - 训练进度和日志会实时显示在下方。

3.  **推理 (Inference)**:
    - 在“推理”选项卡中，从下拉列表选择一个模型（基座或 LoRA 目录）。
    - 点击“加载模型”。加载成功后，即可在下方的对话框中进行聊天。
    - **分享模型**: 模型加载成功后，点击“分享模型 (Gradio)”按钮，应用会自动启动一个 Web 服务，并在日志区显示一个公网分享链接。

4.  **模型管理 (Manage)**:
    - **合并与导入**:
        - 在“模型管理”选项卡中，选择一个训练好的 LoRA 模型目录。
        - 点击“开始合并与导入”，程序会提示您为即将生成的 Ollama 模型命名。
        - 之后，应用将自动完成“模型合并 -> GGUF 转换 -> Ollama 导入”的全套流程。
    - **转换基座模型**:
        - 此功能可将任意 Hugging Face Hub 上的模型直接转换为 Ollama 格式。

5.  **设置 (Settings)**:
    - 在这里配置您本地 `llama.cpp` 仓库的绝对路径。这是模型转换功能正常运行的前提。